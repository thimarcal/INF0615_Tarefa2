# Note a diferença entre as regras geradas ao alterar os parâmetros
rules1 <- apriori(Groceries, parameter = list(supp = 0.5, conf = 0.9))
rules2 <- apriori(Groceries, parameter = list(supp = 0.028, conf = 0.2))
rules3 <- apriori(Groceries, parameter = list(supp = 0.002, conf = 0.8))
# Inspeciona regras do rules2, ordenando-as pela confiança (exibe no máximo 6 elementos por padrão)
inspect(head(rules2, by = "confidence"))
# Inspeciona regras do rules2, ordenando-as pela confiança (exibe 33 regras)
inspect(head(rules2, by = "confidence", n=33))
itemsets <- eclat(transacoes, parameter = list(supp = 0.1))
regrasEclat <- ruleInduction(itemsets, transacoes, confidence = 0.5)
plot(regrasEclat, method="graph", control=list(type="items"))
library(arulesViz)
plot(regrasEclat, method="graph", control=list(type="items"))
library(datasets)
data("iris")
summary(iris)
iris.pcal <- prcomp(iris[,1:4], scale.=TRUE)
iris.pcal
iris.pca1 <- prcomp(iris[,1:4], scale.=TRUE)
iris.pca1
iris
iris.pca2 <- prcomp(iris[,1:4])
iris.pca2
head(iris.pca1$x[,1:2])
iris.pca1$x[,1:2]
iris.pca1$x[,1:3]
iris.pca3 <- princomp(iris[,1:4],cor=TRUE)
iris.pca3
z = iris.pca1$x[,1:2] %*% t(iris.pca1$rotation[,1:2])
head(z)
head(iris)
head(norm(iris))
summary(iris.pca1)
summary(iris.pca2)
summary(iris.pca3)
plot(iris[,1:2], col=iris[,5])
plot(iris.pca1$x[,1:2], col=iris[,5])
plot(iris.pca2$x[,1:2], col=iris[,5])
plot(iris.pca2$x[,1:3], col=iris[,5])
plot(iris.pca2$x[,1:3], col=iris[,5])
plot(iris.pca1$x[,1:2], col=iris[,5])
plot(iris.pca2$x[,1:2], col=iris[,5])
plot(iris[,1:2], col=iris[,5])
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
head(wine)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
inspect(regras)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
View(regras)
View(transacoes)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
install.packages("kmeans")
summary(iris)
source('~/.active-rstudio-document', echo=TRUE)
cl <- kmeans(iris[,1:4], k=3)
cl <- kmeans(iris[,1:4], 3)
View(cl)
cl
table(cl$cluster, iris$Species)
cl2=kmeans(iris[,1:4],3,nstart=20)
cl2
a <- matrix(nrow = 2, ncol = 3)
a <- c(1:6)
a
b <- c(2:7)
dims(a)
dim(a)
dim(a)  <- c(2,3)
a
dim(b) <- c(3,2)
b
a*b
dim(b) <- c(2,3)
b
a*b
a**b
a%*%b
dim(b) <- c(3,2)
a%*%b
det(a)
c <- matrix(c(1:9), nrow = 3, ncol = )
c <- matrix(c(1:9), nrow = 3, ncol = 3)
c
det(c)
plot(a)
plot(c)
d <- matrix(c(1,0,0,1), nrow = 2, ncol = 2)
d
det(d)
c[4,2] <- 8
c[2,2] <- 8
c
det(c)
source('~/Documents/Studies/Unicamp/MDC/INF-613/Redução2.R', echo=TRUE)
wine.pca <- prcomp(wine[,2:13], scale.=TRUE)
summary(wine.pca)
source('~/Documents/Studies/Unicamp/MDC/INF-613/Redução2.R', echo=TRUE)
library(MASS)
boston.pca <- prcomp(Boston, scale.=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
boston.reducedC <- boston.pca$x[, 1:5]
boston.reducedD <- boston.pca$x[, 1:10]
summary(boston.reducedA)
summary(boston.pca)
View(boston.pca)
View(boston.reducedB)
summary(boston.reducedB)
source('~/.active-rstudio-document', echo=TRUE)
summary(boston.reducedA)
summary(boston.reducedB)
summary(boston.reducedC)
summary(boston.reducedD)
summary(boston.pca)
source('~/.active-rstudio-document', echo=TRUE)
summary(boston.reducedA)
summary(boston.reducedB)
summary(boston.reducedC)
summary(boston.reducedD)
set.seed(1234)
boston.reducedA2 <- boston.pca$x[,8]
boston.reducedB2 <- boston.pca$x[, 1:8]
boston.reducedC2 <- boston.pca$x[, 1:5]
boston.reducedD2 <- boston.pca$x[, 1:10]
r5 <- kmeans(Boston, 5)
set.seed(1234)
r8 <- kmeans(Boston, 8)
set.seed(1234)
r7 <- kmeans(Boston, 7)
set.seed(1234)
r4 <- kmeans(Boston, 4)
r4
r4$totss
r5$totss
r6$totss
r7$totss
r8$totss
r4
r5
r7
r7$tot.withinss
r4$tot.withinss
r5$tot.withinss
r6$tot.withinss
r8$tot.withinss
r8$withinss
r5$withinss
r4$withinss
r7$withinss
summary(boston.pca)
summary(boston.reducedB)
summary(boston.reducedB2)
boston.reducedA2
boston.reducedB2
summary(boston.pca)
source('~/.active-rstudio-document', echo=TRUE)
dd
heatmap(as.matrix(dd, symm=TRUE))
heatmap(as.matrix(dd),symm=TRUE)
heatmap(as.matrix(dd),symm=TRUE)
h.s=hclust(dd,"single")
plot(h.s,main="single")
h.a=hclust(dd,"ave")
plot(h.a,main="average")
h.w=hclust(dd,"ward.D")
plot(h.w,main="ward")
# Cortando os dendogramas em K clusters
plot(iris2d)
dd=dist(iris2d)
h.s=hclust(dd,"single")
plot(h.s,main="single")
c.s=cutree(h.s,k=3)
# Desenha o dendrograma delimitando os 3 clusters
rect.hclust(h.s,k=3)
plot (h.a, main="single")
rect.hclust(h.a, k=3)
install.packages("factoextra")
install.packages("magrittr")
source('~/.active-rstudio-document', echo=TRUE)
load("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/INF0613_Trabalho_Final/Trabalho_Final.RData")
# Run k-means and k-medoids for the data (5, 10, 15, 20 clusters)
set.seed(123)
rows <- nrow(dados)
amostra <- dados[sample(rows, rows*0.25),]
#features.kmeans <- kmeans(dados, 5)
amostra.kmeans <- kmeans(amostra, 5)
# Calculatte Silhouette Coeficient
#features.silhouette <- silhouette(features.kmeans$cluster, dados)
#summary(features.silhouette)$avg.width
amostra.silhouette <- silhouette(amostra.kmeans$cluster, amostra)
# Libraries
library(cluster)
# Run k-means and k-medoids for the data (5, 10, 15, 20 clusters)
set.seed(123)
#rows <- nrow(dados)
#amostra <- dados[sample(rows, rows*0.25),]
features.kmeans <- kmeans(dados, 5)
d <- dist(dados)
save.image("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/Trabalho_Final_comdist.RData")
# Calculatte Silhouette Coeficient
features.silhouette <- silhouette(features.kmeans$cluster, d)
summary(features.silhouette)$avg.width
install.packages("NLP")
?ngrams
?ngrams()
dados[,1]
dados[1,]
col.names(dados)
dados
headlines
features
col.names(features)
features[1,]
features.kmeans$cluster == 1
bigramas <- list()
cl <- 1
bigramas$cl <- 12
View(bigramas)
bigramas[cl] <- 12
View(bigramas)
bigramas[cl] <- 14
View(bigramas)
# Calculate the bi-grams using NLP's ngrams method
bigramas <- list()
for (cl in c(1:5)) {
words <- features[1,features.kmeans$cluster == cl]
bigramas[[cl]] <- ngrams(words, 2)
}
features[1,]
features[1,][features.kmeans$cluster == 1]
(features[1,])[features.kmeans$cluster == 1]
words <- features[1,]
View(words)
words <- as.array(words)
words <- c(words)
View(features)
colnames(features)
words <- colnames(features)[features.kmeans$cluster == cl]
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigramas[[cl]] <- ngrams(words, 2)
}
library(NLP)
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigramas[[cl]] <- ngrams(words, 2)
}
View(bigramas)
install.packages("DMwR")
install.packages("dprep")
#install.packages("DMwR")
library(DMwR)
df <- iris[, 1:4]
scores <- lofactor(df, k=5)
outliers <- order(scores, decreasing=T)[1:5]
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
# Escolhendo o limiar
outliers <- which(scores 1.6)
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
# Escolhendo o limiar
outliers <- which(scores=1.6)
# Escolhendo o limiar
outliers <- which(scores, 1.6)
# Escolhendo o limiar
outliers <- which(scores > 1.6)
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
set.seed(1234)
df <- iris[, 1:4]
r <- kmeans(df, 3)
centers <- r$centers[r$cluster,]
distances <- sqrt(rowSums((df - centers)^2))
scores <- distances
outliers <- order(scores, decreasing=T)[1:5]
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
set.seed(1234)
df <- iris[, 1:4]
r <- kmeans(df, 3)
centers <- r$centers[r$cluster,]
distances <- sqrt(rowSums((df - centers)^2))
# mcr: mean cluster radius
mcr <- ave(distances, r$cluster, FUN=mean)
scores <- distances / mcr
outliers <- order(scores, decreasing=T)[1:5]
# Para visualizar
xy <- prcomp(df)$x[, 1:2]
pch <- rep(".", nrow(xy))
pch[outliers] <- "+"
plot(xy, pch=pch)
source('~/.active-rstudio-document', echo=TRUE)
intersect(outliers1, outliers2)
scores3 <- (scores1 + scores2)/2
outliers3 <- order(scores3, decreasing=TRUE)[1:10]
intersect(union(outliers1, outliers2), outliers3)
View(features.kmeans)
features.kmeans$cluster
features.kmeans$cluster == 1
bigramas[[1]]
bigramas[[1]][1]
is.element(bigramas[[1]])
is.element(bigramas[[1]], NA)
is.na(bigramas[[1]])
View(bigramas)
is.element(bigramas[[1]], 'NA')
bigramas[[1]][[1]
]
bigramas[[1]][[1]][1]
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigram <- ngrams(words, 2)
bigramas[[cl]] <- c(bigram[[1]][1], bigram[[1]][2])
}
library(NLP)
# Calculate the bi-grams using NLP's ngrams method
bigramas <- list()
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigram <- ngrams(words, 2)
bigramas[[cl]] <- c(bigram[[1]][1], bigram[[1]][2])
}
View(bigram)
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigramas[[cl]] <- ngrams(words, 2)
}
# Calculate the bi-grams using NLP's ngrams method
bigramas <- list()
for (cl in c(1:5)) {
words <- colnames(features)[features.kmeans$cluster == cl]
bigram <- ngrams(words, 2)
bigramas[[cl]] <- c(bigram[[1]][1], bigram[[1]][2])
}
View(bigramas)
View(bigram)
words
colnames(features)
features.kmeans$cluster == 1
sum(features.kmeans$cluster == 1)
colnames(features)
ncol(features.kmeans$cluster)
View(features.kmeans)
sqrt(312487500)
nrow(dados)
ncol(dados)
25000*1654
# Run k-means and k-medoids for the data (5, 10, 15, 20 clusters)
set.seed(1234)
features.kmeansk5 <- kmeans(dados, 5)
set.seed(1234)
features.kmeansk10 <- kmeans(dados, 10)
set.seed(1234)
features.kmeansk15 <- kmeans(dados, 15)
set.seed(1234)
features.kmeansk20 <- kmeans(dados, 20)
# Calculatte Silhouette Coeficient
features.silhouettek5 <- silhouette(features.kmeansk5$cluster, d)
# Libraries
library(cluster)
library(NLP)
# Calculatte Silhouette Coeficient
features.silhouettek5 <- silhouette(features.kmeansk5$cluster, d)
features.silhouettek10 <- silhouette(features.kmeans10$cluster, d)
features.silhouettek10 <- silhouette(features.kmeansk10$cluster, d)
features.silhouettek15 <- silhouette(features.kmeansk15$cluster, d)
features.silhouettek20 <- silhouette(features.kmeansk20$cluster, d)
View(features.kmeansk15)
# Erros Quadraticos
print("Erros Quadráticos")
print("5")
features.kmeansk5$tot.withinss
print("10")
features.kmeansk10$tot.withinss
print("15")
features.kmeansk15$tot.withinss
print("20")
features.kmeansk20$tot.withinss
# Coeficientes de Silhueta
print("Coeficientes de Silhueta")
print("5")
summary(features.silhouettek5)$avg.width
print("10")
summary(features.silhouettek10)$avg.width
print("15")
summary(features.silhouettek15)$avg.width
print("20")
summary(features.silhouettek20)$avg.width
# Libraries
library(cluster)
library(NLP)
library(flexclust)
library(wordcloud)
library(wordcloud2)
load("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/INF0613_Trabalho_Final/final3.RData")
K <- 20
MOST_FRQ_BIGRAM <- 3
clust_data <- features.kmeans.k20$cluster
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
# Libraries
library(cluster)
library(NLP)
library(flexclust)
library(wordcloud)
library(wordcloud2)
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
install.packages("tm")
library(tm)
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "\\'")
docs <- tm_map(docs, removeNumbers)
headlines$content <- docs$content
load("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/INF0613_Trabalho_Final/final4.RData")
MOST_FRQ_BIGRAM <- 3
clust_data <- features.kmeans.k20$cluster
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, removeNumbers)
headlines$content <- docs$content
# pegar somente do mesmo cluster
for (i in 1:K) {
texts <- headlines$content[clust_data == i]     # k-means
res <- sapply(strsplit(texts, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print(word_freq[1:MOST_FRQ_BIGRAM])
word_freq_df <- as.data.frame(word_freq)
#png(paste("wordcloud_", i, ".png", sep = ""))
wordcloud2(word_freq_df[1:20,])
}
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "\\'")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument)
#docs <- tm_map(docs, removeWords, c("brisban", "new zealand", "australia", "australian"))
docs <- tm_map(docs, removeWords, c("to", "he", "a", "in", "the", "one"))
headlines$content <- docs$content
# pegar somente do mesmo cluster
for (i in 1:K) {
texts <- headlines$content[clust_data == i]     # k-means
res <- sapply(strsplit(texts, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print(i)
print(word_freq[1:MOST_FRQ_BIGRAM])
word_freq_df <- as.data.frame(word_freq)
wordcloud2(word_freq_df)
}
# coletar somente os dados de 2016
for (i in 1:K) {
texts <- headlines[(clust_data == i), ]
texts <- texts[(texts$year == 2016), ]     # k-means
res <- sapply(strsplit(texts$content, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print (i)
print(word_freq[1:MOST_FRQ_BIGRAM])
#word_freq_df <- as.data.frame(word_freq)
#wordcloud2(word_freq_df)
}
library(Rserve)
Rserve(args="--no-save")
source('~/Documents/Studies/Unicamp/MDC/INF-614/iris.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
# Normalize the data (Removing quality)
meanTrainFeatures = colMeans(train_data[,-12)]) #mean of each feature
# Normalize the data (Removing quality)
meanTrainFeatures = colMeans(train_data[,-12]) #mean of each feature
stdTrainFeatures = apply(train_data[,-12], 2, sd) #std of each feature
meanTrainFeatures
stdTrainFeatures
train_data[,-12] = sweep(train_data[,-12], 2, meanTrainFeatures, "-")
train_data[,-12] = sweep(train_data[,-12], 2, meanTrainFeatures, "/")
val_data[,-12] = sweep(val_data[,-12], 2, meanTrainFeatures, "-")
val_data[,-12] = sweep(val_data[,-12], 2, meanTrainFeatures, "/")
summary(train_data)
summary(val_data)
