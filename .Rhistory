print("5")
features.kmeansk5$tot.withinss
print("10")
features.kmeansk10$tot.withinss
print("15")
features.kmeansk15$tot.withinss
print("20")
features.kmeansk20$tot.withinss
# Coeficientes de Silhueta
print("Coeficientes de Silhueta")
print("5")
summary(features.silhouettek5)$avg.width
print("10")
summary(features.silhouettek10)$avg.width
print("15")
summary(features.silhouettek15)$avg.width
print("20")
summary(features.silhouettek20)$avg.width
# Libraries
library(cluster)
library(NLP)
library(flexclust)
library(wordcloud)
library(wordcloud2)
load("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/INF0613_Trabalho_Final/final3.RData")
K <- 20
MOST_FRQ_BIGRAM <- 3
clust_data <- features.kmeans.k20$cluster
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
# Libraries
library(cluster)
library(NLP)
library(flexclust)
library(wordcloud)
library(wordcloud2)
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
install.packages("tm")
library(tm)
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "\\'")
docs <- tm_map(docs, removeNumbers)
headlines$content <- docs$content
load("~/Documents/Studies/Unicamp/MDC/INF-613/Tarefas/Final/INF0613_Trabalho_Final/final4.RData")
MOST_FRQ_BIGRAM <- 3
clust_data <- features.kmeans.k20$cluster
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, removeNumbers)
headlines$content <- docs$content
# pegar somente do mesmo cluster
for (i in 1:K) {
texts <- headlines$content[clust_data == i]     # k-means
res <- sapply(strsplit(texts, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print(word_freq[1:MOST_FRQ_BIGRAM])
word_freq_df <- as.data.frame(word_freq)
#png(paste("wordcloud_", i, ".png", sep = ""))
wordcloud2(word_freq_df[1:20,])
}
#Clean data
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- Corpus(VectorSource(headlines$headline_text1))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
docs <- tm_map(docs, toSpace, "\\'")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, stemDocument)
#docs <- tm_map(docs, removeWords, c("brisban", "new zealand", "australia", "australian"))
docs <- tm_map(docs, removeWords, c("to", "he", "a", "in", "the", "one"))
headlines$content <- docs$content
# pegar somente do mesmo cluster
for (i in 1:K) {
texts <- headlines$content[clust_data == i]     # k-means
res <- sapply(strsplit(texts, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print(i)
print(word_freq[1:MOST_FRQ_BIGRAM])
word_freq_df <- as.data.frame(word_freq)
wordcloud2(word_freq_df)
}
# coletar somente os dados de 2016
for (i in 1:K) {
texts <- headlines[(clust_data == i), ]
texts <- texts[(texts$year == 2016), ]     # k-means
res <- sapply(strsplit(texts$content, fixed =T, split = " "), function(x) vapply(ngrams(x, 2L), paste, "", collapse = " "))
word_freq <- sort(table(unlist(res)), decreasing = T)
print (i)
print(word_freq[1:MOST_FRQ_BIGRAM])
#word_freq_df <- as.data.frame(word_freq)
#wordcloud2(word_freq_df)
}
library(Rserve)
Rserve(args="--no-save")
source('~/Documents/Studies/Unicamp/MDC/INF-614/iris.R', echo=TRUE)
a = 9
b = 5
s <- -a*log2(a) - b*log2(b)
s <- (-a/(a+b))*log2(a) - (b/(a+b))*log2(b)
s <- (-a/(a+b))*log2(-a/(a+b)) - (b/(a+b))*log2(b/(a+b))
pmais <- 9/14
pmenos <- 5/14
s<- -pmais*log2(pmais) - pmenos*log2(pmenos)
########
pmais <- 3/7
pmenos <- 4/7
s<- -pmais*log2(pmais) - pmenos*log2(pmenos)
###########
pmais <- 6/7
pmenos <- 1/7
s<- -pmais*log2(pmais) - pmenos*log2(pmenos)
gain <- s_um - (7/14)*s_h - (7/14)*s_l
source('~/.active-rstudio-document', echo=TRUE)
sunny_mais <- 2/5
sunny_menos <- 3/5
s_sunny <- -sunny_mais*log2(sunny_mais) - sunny_menos*log2(sunny_menos)
umid_alta_mais <- 0/3
umid_alta_menos <- 3/3
s_um_alta <- -umid_alta_menos*log2(umid_alta_menos)
#################################
# MDC - Machine Learning		#
# DT & RF						#
# Income Census					#
#################################
install.packages('randomForest')
source('~/Documents/Studies/Unicamp/MDC/INF-615/ExercÃ­cios/03/treeToDo.R', echo=TRUE)
summary(trainData)
#If we want to use Entropy + Gain of Information
#### -------> TODO
treeModel <- rpart(formula = class ~., data = trainData, parms = list(split="information"))
#If we want to use Entropy + Gain of Information
#### -------> TODO
treeModel <- rpart(formula = class ~., data = trainData, parms = list(split="information"), method = "class")
summary(treeModel)
#Plot DT
plot(treeModel, uniform=TRUE)
text(treeModel, use.n=TRUE, all=TRUE, cex=.8)
#Save the complete DT into file
post(treeModel, file = "tree.ps",title = "Classification Tree for Income")
#If we want to use Gini to select features
#### -------> TODO
treeModelGini <- rpart(formula = class ~., data = trainData, parms = list(split="gini"), method = "class")
summary(treeModel)
summary(treeModelGini)
#Plot DT
plot(treeModel, uniform=TRUE)
text(treeModel, use.n=TRUE, all=TRUE, cex=.8)
#Save the complete DT into file
post(treeModel, file = "tree.ps",title = "Classification Tree for Income")
#Plot DT GINI
plot(treeModelGini, uniform=TRUE)
text(treeModelGini, use.n=TRUE, all=TRUE, cex=.8)
#Save the complete DT into file
post(treeModelGini, file = "treeGini.ps",title = "Classification Tree for Income")
#Allowing it to grow
treeModelGrow = rpart(formula=class~ .,
data=trainData, method="class",
control=rpart.control(minsplit=10, cp=0.0001),
parms= list(split="information"))
summary(treeModel)
summary(treeModelGini)
summary(treeModelGrow)
#Plot DT
plot(treeModel, uniform=TRUE)
text(treeModel, use.n=TRUE, all=TRUE, cex=.8)
#Save the complete DT into file
post(treeModel, file = "tree.ps",title = "Classification Tree for Income")
#Plot DT GINI
plot(treeModelGini, uniform=TRUE)
text(treeModelGini, use.n=TRUE, all=TRUE, cex=.8)
#Save the complete DT into file
post(treeModelGini, file = "treeGini.ps",title = "Classification Tree for Income")
#Plot DT GINI
plot(treeModelGrow, uniform=TRUE)
text(treeModelGrow, use.n=TRUE, all=TRUE, cex=.8)
#Save the complete DT into file
post(treeModelGrow, file = "treeGini.ps",title = "Classification Tree for Income")
#Save the complete DT into file
post(treeModelGini, file = "treeGini.ps",title = "Classification Tree for Income")
#Plot DT GINI
plot(treeModelGrow, uniform=TRUE)
text(treeModelGrow, use.n=TRUE, all=TRUE, cex=.8)
#Save the complete DT into file
post(treeModelGrow, file = "treeGrow.ps",title = "Classification Tree for Income")
#Save the complete DT into file
post(treeModelGrow, file = "treeGrow.ps",title = "Classification Tree for Income")
#Print the table with complexity parameters
printcp(treeModelGrow)
plot(treeModelGrow$cptable$xerror)
plot(treeModelGrow$cptable[4])
plot(treeModelGrow$cptable[,4])
min(treeModelGrow$cptable[,4])
#Prune the tree based on the complexity parameter that minimizes
#the error in cross-validation (xerror)
minCP = treeModelGrow$cptable[which.min(treeModelGrow$cptable[,"xerror"]),"CP"]
minCP
ptree = prune(treeModelGrow, cp=minCP)
summary(ptree)
#Plot the pruned tree
plot(ptree, uniform=TRUE)
text(ptree, use.n=TRUE, all=TRUE, cex=.8)
#Save the complete DT into file
post(ptree, file = "pruned-tree.ps",title = "Prunned Classification Tree for Income")
#Let's see how we do in the valSet
#non-prunned tree
treeEval = predictAndEvaluate(treeModel, valData)
treeEval$CM
treeEval$ACCNorm
#Let's see how we do in the valSet
#non-prunned tree
treeEvalGini = predictAndEvaluate(treeModelGini, valData)
treeEvalGini$CM
treeEvalGini$ACCNorm
#Let's see how we do in the valSet
#non-prunned tree
treeEvalGrow = predictAndEvaluate(treeModelGrow, valData)
treeEvalGrow$CM
treeEvalGrow$ACCNorm
#prunned tree
ptreeEval = predictAndEvaluate(ptree, valData)
ptreeEval$CM
ptreeEval$ACCNorm
########## ACC Vs Depth
# Let's see how the acc varies as we increase the tree's depth
accPerDepth = data.frame(depth=numeric(30), accTrain=numeric(30), accVal=numeric(30))
for (maxDepth in 3:30){
#### -------> TODO
trainResults = predictAndEvaluate(treeModel, trainData)
valResults = predictAndEvaluate(treeModel, valData)
accPerDepth[maxDepth,] = c(maxDepth, trainResults$ACCNorm, valResults$ACCNorm)
}
for (maxDepth in 3:30){
#### -------> TODO
treeModel = rpart(formula=class~ .,
data=trainData, method="class",
control=rpart.control(minsplit=10, cp=0.0001, maxdepth=maxDepth),
parms= list(split="information"))
trainResults = predictAndEvaluate(treeModel, trainData)
valResults = predictAndEvaluate(treeModel, valData)
accPerDepth[maxDepth,] = c(maxDepth, trainResults$ACCNorm, valResults$ACCNorm)
}
#Plot
library("reshape2")
library("ggplot2")
accPerDepth <- melt(accPerDepth, id="depth")  # convert to long format
ggplot(data=accPerDepth, aes(x=depth, y=value, colour=variable)) + geom_line()
############# RANDOM FOREST
install.packages('randomForest')
library(randomForest)
############# RANDOM FOREST
#install.packages('randomForest')
library(randomForest)
#Plotting the error
layout(matrix(c(1,2),nrow=1), width=c(4,1))
#Train RF model
#### -------> TODO
rfModel <- randomForest(formula = class ~ ., data = trainData, nTress=100)
#Plotting the error
layout(matrix(c(1,2),nrow=1), width=c(4,1))
par(mar=c(5,4,4,0)) #No margin on the right side
plot(rfModel, log="y")
#Train RF model
#### -------> TODO
rfModel <- randomForest(formula = class ~ ., data = trainData, ntree=100)
#Plotting the error
layout(matrix(c(1,2),nrow=1), width=c(4,1))
par(mar=c(5,4,4,0)) #No margin on the right side
plot(rfModel, log="y")
par(mar=c(5,0,4,2)) #No margin on the left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(rfModel$err.rate),col=1:4,cex=0.8,fill=1:4)
#Confusion Matrix
rfPrediction = predict(rfModel, valData)
rfCM = as.matrix(table(Actual = valData$class, Predicted = rfPrediction))
rfTPR = rfCM[2,2] / (rfCM[2,2] + rfCM[2,1])
rfTNR = rfCM[1,1] / (rfCM[1,1] + rfCM[1,2])
rfACCNorm = mean(c(rfTPR, rfTNR))
rfACCNorm
#Train RF model
#### -------> TODO
rfModel <- randomForest(formula = class ~ ., data = trainData, ntree=1000)
#Plotting the error
layout(matrix(c(1,2),nrow=1), width=c(4,1))
par(mar=c(5,4,4,0)) #No margin on the right side
plot(rfModel, log="y")
par(mar=c(5,0,4,2)) #No margin on the left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(rfModel$err.rate),col=1:4,cex=0.8,fill=1:4)
#Confusion Matrix
rfPrediction = predict(rfModel, valData)
rfCM = as.matrix(table(Actual = valData$class, Predicted = rfPrediction))
rfTPR = rfCM[2,2] / (rfCM[2,2] + rfCM[2,1])
rfTNR = rfCM[1,1] / (rfCM[1,1] + rfCM[1,2])
rfACCNorm = mean(c(rfTPR, rfTNR))
rfACCNorm
#Train RF model
#### -------> TODO
rfModel <- randomForest(formula = class ~ ., data = trainData, ntree=50)
#Plotting the error
layout(matrix(c(1,2),nrow=1), width=c(4,1))
par(mar=c(5,4,4,0)) #No margin on the right side
plot(rfModel, log="y")
par(mar=c(5,0,4,2)) #No margin on the left side
plot(c(0,1),type="n", axes=F, xlab="", ylab="")
legend("top", colnames(rfModel$err.rate),col=1:4,cex=0.8,fill=1:4)
#Confusion Matrix
rfPrediction = predict(rfModel, valData)
rfCM = as.matrix(table(Actual = valData$class, Predicted = rfPrediction))
rfTPR = rfCM[2,2] / (rfCM[2,2] + rfCM[2,1])
rfTNR = rfCM[1,1] / (rfCM[1,1] + rfCM[1,2])
rfACCNorm = mean(c(rfTPR, rfTNR))
rfACCNorm
nTree = c(10,25, 50, 100, 500)
accPerNTree = data.frame(ntree=numeric(5), accTrain=numeric(5))
for (i in 1:5){
rfModel = randomForest(formula=class~ ., data= trainData, ntree=nTree[i])
rfPrediction = predict(rfModel, valData)
rfCM = as.matrix(table(Actual = valData$class, Predicted = rfPrediction))
rfTPR = rfCM[2,2] / (rfCM[2,2] + rfCM[2,1])
rfTNR = rfCM[1,1] / (rfCM[1,1] + rfCM[1,2])
rfACCNorm = mean(c(rfTPR, rfTNR))
accPerNTree[i,] = c(nTree[i], rfACCNorm)
}
install.packages("ipred")
bagging?
;
?bagging
?plot
?bagging()
?bagging
help(bagging)
?ipredbagg
## Bagging
library(ipred)
?bagging
model = ipredbagg(trainData$class, X=trainData[,1:14], nbagg=50)
baggPrediction = predict(model, valData)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa2/Trabalho2.R', echo=TRUE)
levels(train_data$quality)
sum(train_data$quality)
count(train_data$quality)
sum(train_data, train_data$quality == 1)
train_data$quality
train_data$quality == 1
sum(train_data$quality)
sum(train_data$quality == 1)
sum(train_data$quality == 0)
train_data[1,]
train_data[train_data$quality==1,]
train_data[,train_data$quality==1]
train_data[train_data$quality==0,]
sample(train_data[train_data$quality == 0,], size = sum(train_data$quality == 1))
sample(100, 10)
sample(train_data, 711)
bad_wines <- train_data[train_data$quality==0,]
sample_wines <- sample(1:nrow(bad_wines), sum(train_data$quality==1))
bad_wines <- bad_wines[sample_wines,]
balanced_win
balanced_wines <- bad_wines + train_data[train_data$quality==1,]
balanced_wines <- bad_wines
?rbind
rbind(balanced_wines, train_data[train_data$quality==1])
rbind(balanced_wines, train_data[train_data$quality==1,])
balanced_wines <- rbind(balanced_wines, train_data[train_data$quality==1,])
#############################################################
# Balance the Data - Process 1 - decrease the higher        #
#############################################################
bad_wines <- train_data[train_data$quality==0,]
sample_wines <- sample(1:nrow(bad_wines), sum(train_data$quality==1))
bad_wines <- bad_wines[sample_wines,]
balanced_train <- rbind(balanced_wines, train_data[train_data$quality==1,])
balanced_train <- rbind(bad_wines, train_data[train_data$quality==1,])
logistic.train(balanced_train, val_data)
install.packages("DMwR")
smoted_data <- SMOTE(quality~., train_data, perc.over=100)
)
install.packages("DMwR")
library(DMwR)
smoted_data <- SMOTE(quality~., train_data, perc.over=100)
sum(smoted_data$quality == 1)
sum(smoted_data$quality == 0)
?SMOTE
smoted_data <- SMOTE(quality~., train_data, perc.over=200, perc.under = 100)
smoted_data <- SMOTE(quality~., train_data, perc.over=700, perc.under = 100)
smoted_data <- SMOTE(quality~., train_data, perc.over=300, perc.under = 100)
sum(smoted_data$quality == 0)
sum(smoted_data$quality == 1)
smoted_data <- SMOTE(quality~., train_data, perc.over=400, perc.under = 100)
sum(smoted_data$quality == 1)
sum(smoted_data$quality == 0)
smoted_data <- SMOTE(quality~., train_data, perc.over=350, perc.under = 100)
sum(smoted_data$quality == 1)
sum(smoted_data$quality == 0)
smoted_data <- SMOTE(quality~., train_data, perc.over=400, perc.under = 200)
sum(smoted_data$quality == 0)
sum(smoted_data$quality == 1)
smoted_data <- SMOTE(quality~., train_data, perc.over=400, perc.under = 120)
sum(smoted_data$quality == 1)
sum(smoted_data$quality == 0)
smoted_data <- SMOTE(quality~., train_data, perc.over=380, perc.under = 120)
sum(smoted_data$quality == 1)
sum(smoted_data$quality == 0)
smoted_data <- SMOTE(quality~., train_data, perc.over=400, perc.under = 130)
sum(smoted_data$quality == 0)
sum(smoted_data$quality == 1)
smoted_data <- SMOTE(quality~., train_data, perc.over=400, perc.under = 125)
sum(smoted_data$quality == 1)
sum(smoted_data$quality == 0)
smoted_data <- SMOTE(quality~., train_data, perc.over=400, perc.under = 125)
logistic.train(smoted_data, val_data)
#############################################################
# Remove Outliers from Original data                        #
#############################################################
train_data <- read.csv("wineQuality_train.data", header = TRUE)
val_data<- read.csv("wineQuality_val.data", header = TRUE)
dim(train_data)
summary(train_data)
sum(train_data$fixed.acidity > 15.000)
sum(train_data$fixed.acidity > 13.000)
sum(train_data$fixed.acidity > 10.000)
mean(train_data$fixed.acidity)
sd(train_data$fixed.acidity)
sum(train_data$fixed.acidity > (mean(train_data$fixed.acidity) + 2*sd(train_data$fixed.acidity)) )
sum(train_data$fixed.acidity < (mean(train_data$fixed.acidity) - 2*sd(train_data$fixed.acidity)) )
# Remove Outliers?
# Max Residual sugar 31.600, 3rd quarter 8.300
# Max free.sulfur.dioxide 131.00 3rd quarter 41.00
# total.sulfur.dioxide min and max seem out of normal range
mean_acidity <- mean(train_data$fixed.acidity)
sd_acidity <- sd(train_data$fixed.acidity)
train_data <- train_data[train_data < (mean_acidity + 2*sd_acidity), ]
train_data <- train_data[train_data > (mean_acidity - 2*sd_acidity), ]
summary(train_data)
#############################################################
# Remove Outliers from Original data                        #
#############################################################
train_data <- read.csv("wineQuality_train.data", header = TRUE)
dim(train_data)
summary(train_data)
# Remove Outliers?
# Max Residual sugar 31.600, 3rd quarter 8.300
# Max free.sulfur.dioxide 131.00 3rd quarter 41.00
# total.sulfur.dioxide min and max seem out of normal range
mean_acidity <- mean(train_data$fixed.acidity)
sd_acidity <- sd(train_data$fixed.acidity)
train_data <- train_data[train_data$fixed.acidity < (mean_acidity + 2*sd_acidity), ]
train_data <- train_data[train_data$fixed.acidity > (mean_acidity - 2*sd_acidity), ]
summary(train_data)
mean(train_data$residual.sugar)
sd(train_data$residual.sugar)
sd(train_data$residual.sugar)
#
# Still outliers in Residual Sugar
mean_sugar <- mean(train_data$residual.sugar)
sd_sugar <- sd(train_data$residual.sugar)
train_data <- train_data[train_data$residual.sugar < (mean_sugar + 2*sd_sugar), ]
train_data <- train_data[train_data$residual.sugar > (mean_sugar - 2*sd_sugar), ]
summary(train_data)
# The same process for Free Sulfur Dioxide
mean_sulfur <- mean(train_data$free.sulfur.dioxide)
sd_sulfur <- sd(train_data$free.sulfur.dioxide)
train_data <- train_data[train_data$free.sulfur.dioxide < (mean_sulfur + 2*sd_sulfur), ]
train_data <- train_data[train_data$free.sulfur.dioxide > (mean_sulfur - 2*sd_sulfur), ]
summary(train_data)
# Normalize the data (Removing quality)
meanTrainFeatures = colMeans(train_data[,-12]) #mean of each feature
stdTrainFeatures = apply(train_data[,-12], 2, sd) #std of each feature
meanTrainFeatures
stdTrainFeatures
train_data[,-12] = sweep(train_data[,-12], 2, meanTrainFeatures, "-")
train_data[,-12] = sweep(train_data[,-12], 2, meanTrainFeatures, "/")
val_data[,-12] = sweep(val_data[,-12], 2, meanTrainFeatures, "-")
val_data[,-12] = sweep(val_data[,-12], 2, meanTrainFeatures, "/")
train_data$quality <- as.factor(train_data$quality)
val_data$quality <- as.factor(val_data$quality)
summary(train_data)
summary(val_data)
cor(train_data[,-12])
pairs(~., data = train_data[,-12])
logistic.train(train_data, val_data)
#############################################################
# Balance the Data - Process 1 - decrease the higher        #
#############################################################
bad_wines <- train_data[train_data$quality==0,]
sample_wines <- sample(1:nrow(bad_wines), sum(train_data$quality==1))
bad_wines <- bad_wines[sample_wines,]
balanced_train <- rbind(bad_wines, train_data[train_data$quality==1,])
logistic.train(balanced_train, val_data)
#############################################################
# Balance the Data - Process 2 - create sample data         #
#############################################################
library(DMwR)
smoted_data <- SMOTE(quality~., train_data, perc.over=400, perc.under = 125)
logistic.train(smoted_data, val_data)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa2/Trabalho2.R', echo=TRUE)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa2/Trabalho2.R', echo=TRUE)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa2/Trabalho2.R', echo=TRUE)
source('~/Documents/Studies/Unicamp/MDC/INF-615/Tarefas/INF0615_Tarefa2/Trabalho2.R', echo=TRUE)
